{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first project of the Greedy-tech NLP Camp. Our goal is to understand each part of the NLP task and build a retrieval-based QA system.\n",
    "The organization of this project is as follows:\n",
    "Part 1: build a word segmentation tool with the enumeration method\n",
    "part 2: apply the Viterbi algorithm to decrease the time complexity\n",
    "part 3: understand the dataset, which involves the question-answer pair\n",
    "part 4: text preprocessing\n",
    "part 5: word representation with tf-idf method\n",
    "part 6: find the top 5 answers to a testing question by comparing the testing question's cosine similarity with the questions in the data set\n",
    "part 7: apply the inverted index to decrease the time complexity\n",
    "part 8: replace the tf-idf word representation with the word2vec method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 1: build a word segmentation tool with the enumeration method \n",
    "The given data includes:\n",
    "1) one Chinese dictionary, dic.xlsx\n",
    "2) part of the words' probability. For other words' probability, if the word exists in the dictionary, then the probability is 0.00001, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# use set to store the dictionary and check the given probability\n",
    "import pandas as pd\n",
    "dic_words = pd.read_excel('data\\dic.xlsx',header=None)   \n",
    "dic_set = set(dic_words[0])\n",
    "print('羊肚子' in dic_set)\n",
    "\n",
    "word_prob = {\"北京\":0.03,\"的\":0.08,\"天\":0.005,\"气\":0.005,\"天气\":0.06,\"真\":0.04,\"好\":0.05,\"真好\":0.04,\"啊\":0.01,\"真好啊\":0.02, \n",
    "             \"今\":0.01,\"今天\":0.07,\"课程\":0.06,\"内容\":0.06,\"有\":0.05,\"很\":0.03,\"很有\":0.04,\"意思\":0.06,\"有意思\":0.005,\"课\":0.01,\n",
    "             \"程\":0.005,\"经常\":0.08,\"意见\":0.08,\"意\":0.01,\"见\":0.005,\"有意见\":0.02,\"分歧\":0.04,\"分\":0.02, \"歧\":0.005}\n",
    "\n",
    "print (sum(word_prob.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a naive word segmentation tool and check the results\n",
    "import math\n",
    "def word_segment_naive(input_str):\n",
    "    \"\"\"\n",
    "    input chinese words and output its best segmentation.\n",
    "    use dfs to do exploration search\n",
    "    \"\"\"\n",
    "    best_score = 1e10       \n",
    "    segments = helper(input_str)\n",
    "    for st in segments:\n",
    "        score = 0\n",
    "        for each in st:\n",
    "            score -= math.log(word_prob.get(each,0.00001))\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_segment = st  \n",
    "\n",
    "    return best_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(input_str):\n",
    "    if(len(input_str) == 0):\n",
    "        return[[]]\n",
    "    else:\n",
    "        res = []\n",
    "        for i in range(1,len(input_str)+1):\n",
    "            if(input_str[:i] in dic_set):\n",
    "                tmp = helper(input_str[i:])\n",
    "                for j in tmp:\n",
    "                    res.append([input_str[:i]] + j)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['今', '天', '天', '气', '好'],\n",
       " ['今', '天', '天气', '好'],\n",
       " ['今', '天天', '气', '好'],\n",
       " ['今天', '天', '气', '好'],\n",
       " ['今天', '天气', '好']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper(\"今天天气好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '天气', '好']\n",
      "['今天', '的', '课程', '内容', '很', '有意思']\n",
      "['经常', '有', '意见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(word_segment_naive(\"今天天气好\"))\n",
    "print(word_segment_naive(\"今天的课程内容很有意思\"))\n",
    "print(word_segment_naive(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time complexity: O(2^n), space complexity: O(2^n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 2: apply the Viterbi algorithm to decrease the time complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob.update(\n",
    "{word:0.00001 for word in dic_set if word not in word_prob.keys()}\n",
    ")\n",
    "word_score = {word: -math.log(word_prob[word]) for word in word_prob.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def get_graph(input_str):\n",
    "    graph = collections.OrderedDict()\n",
    "    for i in range(len(input_str) + 1):\n",
    "        if i not in graph: \n",
    "            graph[i] = {}\n",
    "        for j in range(i):\n",
    "            if input_str[j:i] in word_score.keys():\n",
    "                graph[i][j] = word_score[input_str[j:i]]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, {}), (1, {0: 4.605170185988091}), (2, {0: 2.659260036932778, 1: 5.298317366548036}), (3, {1: 11.512925464970229, 2: 5.298317366548036}), (4, {2: 2.8134107167600364, 3: 5.298317366548036}), (5, {4: 2.995732273553991})])\n"
     ]
    }
   ],
   "source": [
    "print(get_graph(\"今天天气好\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path(graph):\n",
    "    score = {0:0}\n",
    "    path = {}\n",
    "    for i in graph.keys():\n",
    "        if graph[i]:\n",
    "            for j in graph[i].keys():\n",
    "                if i not in score.keys() or score[i] > graph[i][j] + score[j]:\n",
    "                    score[i] = graph[i][j] + score[j]\n",
    "                    path[i] = j\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 0, 3: 2, 4: 2, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "graph = get_graph(\"今天天气好\")\n",
    "print(get_best_path(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_segment(path, input_str):\n",
    "    \"\"\"\n",
    "    input chinese string and output the best segmentation\n",
    "    build the graph first and then use the viterbi to solve it\n",
    "    \"\"\"\n",
    "    segment = []\n",
    "    i = len(input_str)\n",
    "    while i > 0:\n",
    "        segment = [input_str[path[i] : i]] + segment\n",
    "        i = path[i]\n",
    "    #segment.reverse()\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '天气', '好']\n"
     ]
    }
   ],
   "source": [
    "graph = get_graph(\"今天天气好\")\n",
    "path = get_best_path(graph)\n",
    "print(path_to_segment(path, \"今天天气好\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segment_viterbi(input_str):\n",
    "    graph = get_graph(input_str)\n",
    "    path = get_best_path(graph)\n",
    "    segment = path_to_segment(path, input_str)\n",
    "    return segment                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['北京', '的', '天气', '真好啊']\n",
      "['今天', '的', '课程', '内容', '很有', '意思']\n",
      "['经常', '有意见', '分歧']\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(word_segment_viterbi(\"北京的天气真好啊\"))\n",
    "print(word_segment_viterbi(\"今天的课程内容很有意思\"))\n",
    "print(word_segment_viterbi(\"经常有意见分歧\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time complexity: O(n^2), space complexity: O(n^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential further improvement:\n",
    "1) the current probability is calculated based on the collected documents. We can consider more data to get more accurate probability\n",
    "2) one-order markov is nor precise\n",
    "3) it can not deal with polysemy\n",
    "4) if the words do not exist in the dictionary, these words can not be segmented correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 3:understand the dataset, which involves the question-answer pair\n",
    "From this part, we start build our retrieval-based QA system.\n",
    "The given data includes:\n",
    "1) dev-v2.0.json file includes the question-answer pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dwang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dwang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial.distance import cosine as cosdist\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "qa_corpus_path = './data/train-v2.0.json'\n",
    "glove_path = \"./data/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and plot the data\n",
    "def read_corpus():\n",
    "    with open(qa_corpus_path, 'r', encoding='utf-8') as fp:\n",
    "        json_corpus = json.load(fp)['data']\n",
    "    qlist = []; alist = []\n",
    "    for item in json_corpus:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            for qa in para[\"qas\"]:\n",
    "                qlist.append(qa[\"question\"])\n",
    "                try:\n",
    "                    alist.append(qa[\"answers\"][0][\"text\"])\n",
    "                except IndexError:\n",
    "                    qlist.pop()\n",
    "                \n",
    "    assert len(qlist) == len(alist) \n",
    "    return qlist, alist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular?'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlist, alist = read_corpus()\n",
    "qlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the late 1990s'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(textlist):\n",
    "    \"\"\"\n",
    "    input sentence list, and output word dict {word, #counts}\n",
    "    \"\"\"\n",
    "    word_dict = defaultdict(lambda:0)\n",
    "    for text in textlist:\n",
    "        for token in text.split(\" \"):\n",
    "            word_dict[token] += 1\n",
    "    word_dict = sorted(word_dict.items(), key=lambda item:item[1], reverse=True)\n",
    "    return dict(word_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk(n, word_dict):\n",
    "    \"\"\"\n",
    "    return the most frequent n words from the dict\n",
    "    \"\"\"\n",
    "    res=[]\n",
    "    for word, freq in word_dict.items():\n",
    "        res.append(\"{}({})\".format(word,freq))\n",
    "        n-=1;\n",
    "        if n == 0:\n",
    "            return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 903411 words and 63780 distinct tokens\n"
     ]
    }
   ],
   "source": [
    "# data statistics analysis\n",
    "q_dict = get_dict(qlist)\n",
    "word_total_q = sum(q_dict.values())\n",
    "n_distinct_words_q = len(q_dict)\n",
    "print(\"There are {} words and {} distinct tokens\".format(word_total_q, n_distinct_words_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10000 artists>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(10000), list(q_dict.values())[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10000 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(10000), list(q_dict.values())[100:10100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure satisfies the zipf's law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 4: text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "\"\"\"\n",
    "1. we use the lemmatizer instead of the stemming method. Because stemming method\n",
    "is build based on rules while lemmatizer can return the original words.\n",
    "eg:\n",
    "leaves---stemming---leav , usually the returned is not a word\n",
    "leaves---lemmatizer---leaf\n",
    "2. we use #num to replace all tnumbers in the corpus\n",
    "3. transfer all words to starting with the small letters.\n",
    "4. filter the words with stopwords provided by nltk\n",
    "5. remove meanless symbols\n",
    "6. remove low frequency words\n",
    "7. filter the most frequent and lowest frequent words\n",
    "\"\"\"\n",
    "class TextNormalizer:\n",
    "    def __init__(self, stopwords_set, filter_vocab, re_cleaners):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.filter_vocab = filter_vocab\n",
    "        self.stopwords = stopwords_set\n",
    "        self.re_cleaners = re_cleaners\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "    def normalize_texts(self, text_list):\n",
    "        return [self.normalize_text(text) for text in text_list]\n",
    "    def normalize_text(self, text):\n",
    "        for re_pattern in self.re_cleaners:\n",
    "            text = re.sub(re_pattern, \" \", text)\n",
    "        pos_tokens = pos_tag(word_tokenize(text))\n",
    "        lemmatized = []\n",
    "        for w, pos in pos_tokens:\n",
    "            if not w or w in self.stopwords or w in self.filter_vocab: continue\n",
    "            if pos == 'CD':\n",
    "                lemmatized.append(\"#NUM\")\n",
    "                continue\n",
    "            wn_pos = self.get_wordnet_pos(pos)\n",
    "            if wn_pos:\n",
    "                lemmatized.append(self.lemmatizer.lemmatize(w, pos=wn_pos))\n",
    "            else:\n",
    "                lemmatized.append(self.lemmatizer.lemmatize(w))\n",
    "        return \" \".join(lemmatized).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stwset = set(stopwords.words('english'))\n",
    "stwset.update({'-s','-ly','</s>','\\'s','\\'\\'', '\\''})\n",
    "threshold_lower = 1\n",
    "threshold_upper = 1e4\n",
    "re_pats = [r'[?|!|\\'|\"#]',r'[.|,|)|(|\\|/]']\n",
    "filter_words = {w for w, freq in q_dict.items() if freq <= threshold_lower or freq >= threshold_upper}\n",
    "text_normalizer = TextNormalizer(stwset, filter_words, re_pats)\n",
    "qlist = text_normalizer.normalize_texts(qlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.here I choose the threshold to be 1. Because at least 50% of the words appear once. Many odd experssions, symbols and strange characters appear once.\n",
    "2.inorder to keep the readablity of the answer, I do not normalize the answer list here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for word in word_tokenize(text):\n",
    "    word = ps.stem(word.lower())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 5: word representation with tf-idf method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf-idf adv: easy, fast, understandable\\ndisadv: can not consider the context, position, pos and etc\\nsometimes the key words appear one or two times\\nX is stored in sparse matrix form\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word expression with tfidf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(qlist)\n",
    "\"\"\"\n",
    "tf-idf adv: easy, fast, understandable\n",
    "disadv: can not consider the context, position, pos and etc\n",
    "sometimes the key words appear one or two times\n",
    "X is stored in sparse matrix form\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999988142327"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the sparsity\n",
    "sparsity = np.divide(np.prod(X.shape) - len(X.nonzero()), np.prod(X.shape))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 6: find the top 5 answers to a testing question by comparing the testing question's cosine similarity with the questions in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top5 answer\n",
    "def top5results(input_q, K=5):\n",
    "    \"\"\"\n",
    "    input strings, return top5 strings\n",
    "    use cos to measure the similarity\n",
    "    use pq to find kth largest\n",
    "    \"\"\"\n",
    "    if not input_q or type(input_q)!= type(\"\"):\n",
    "        print(\"input error\")\n",
    "        return\n",
    "    input_q = text_normalizer.normalize_text(input_q)\n",
    "    q_vec = vectorizer.transform([input_q]).todense()\n",
    "    top_k_indices, top_idx = [], []\n",
    "    for i in range(X.shape[0]):\n",
    "        similarity = 1 - cosdist(X[i,:].todense(), q_vec)\n",
    "        similarity = 0 if np.isnan(similarity) else similarity\n",
    "        if len(top_k_indices) == K:\n",
    "            heapq.heappushpop(top_k_indices,(similarity, i))\n",
    "        else:\n",
    "            heapq.heappush(top_k_indices, (similarity, i))\n",
    "    top_idxs = sorted(top_k_indices, reverse=True)\n",
    "    _, top_idxs = zip(*top_idxs[::-1])\n",
    "    return [alist[id] for id in top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:714: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andrew Jackson', 'January 20, 1953', 'George Washington', 'President Johnson', 'President Franklin D. Roosevelt']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(top5results(\"who is the president of THE UNITED STATES NOW???\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complexity analysis\n",
    "time O(ND) \n",
    "N refers to the number of questions in the data set D refers to the size of the dictionary We calculate the cosin-similarity with the D time\n",
    "complexity Comparing the querried question with all N questions in data set is in time complexity N\n",
    "space O(1)\n",
    "\n",
    "issues:\n",
    "1. the answer is constrainted by our question-answer pairs\n",
    "2. the word \"now\" has no use\n",
    "3. the time complexity is too high. If the size of the question-answer paris is too large, it cost too much time to calculate cos similarity with each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 7: apply the inverted list to decrease the time complexity.    \n",
    "use inverted list to find candidates first inorder to save the cos similarity time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_idx(text_list):\n",
    "    \"\"\"\n",
    "    output inverted idx dict\n",
    "    \"\"\"\n",
    "    invidx = defaultdict(lambda:set())\n",
    "    for i, text in enumerate(text_list):\n",
    "        for token in text.split(\" \"):\n",
    "            if not token: continue\n",
    "            invidx[token].add(i)\n",
    "    return invidx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_docs(invidx, query):\n",
    "    \"\"\"\n",
    "    find the intersection of document sets of the query \n",
    "    tokens and the inverted dict\n",
    "    \"\"\"\n",
    "    candidate_ids = set()\n",
    "    for token in query.split(\" \"):\n",
    "        docs = invidx.get(token, set())\n",
    "        candidate_ids = candidate_ids.intersection(docs)\n",
    "    if candidate_ids:\n",
    "        return list(candidate_ids)\n",
    "    \n",
    "    for token in query.split(\" \"):\n",
    "        candidate_ids = candidate_ids.union(invidx.get(token, set()))\n",
    "    if candidate_ids:\n",
    "        return list(candidate_ids)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_idx = create_inverted_idx(qlist)\n",
    "def top5results_invidx(input_q, K=5):\n",
    "    \"\"\"\n",
    "    similar to the above one, except we use inverted list to\n",
    "    find the candidate set first.\n",
    "    \"\"\"\n",
    "    if not input_q or type(input_q)!= type(\"\"):\n",
    "        print(\"input error\")\n",
    "        return\n",
    "    input_q = text_normalizer.normalize_text(input_q)\n",
    "    candidate_docs = get_candidate_docs(inverted_idx, input_q)\n",
    "    if not candidate_docs:\n",
    "        print(\"no answer error\")\n",
    "        return\n",
    "    q_vec = vectorizer.transform([input_q]).todense()\n",
    "    top_k_indices, top_idx = [], []\n",
    "    for i in candidate_docs:\n",
    "        doc_vec = X[i,:].todense()\n",
    "        similarity = 1 - cosdist(doc_vec, q_vec)\n",
    "        similarity = 0 if np.isnan(similarity) else similarity\n",
    "        if len(top_k_indices) == K:\n",
    "            heapq.heappushpop(top_k_indices,(similarity, i))\n",
    "        else:\n",
    "            heapq.heappush(top_k_indices, (similarity, i))\n",
    "    top_idxs = sorted(top_k_indices, reverse=True)\n",
    "    _, top_idxs = zip(*top_idxs[::-1])\n",
    "    return [alist[id] for id in top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andrew Jackson', 'January 20, 1953', 'George Washington', 'President Johnson', 'President Franklin D. Roosevelt']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(top5results_invidx(\"who is the president of THE UNITED STATES NOW???\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complexity analysis time O(N_d * D)   N_d is the size of the largest set in the inverted list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 8, use word2vec.                        \n",
    "In the above part, we use the bag-of-words model. But this model can not consider the similarity between words and the representation vector is sparse. In the following part, we use the word2vec model with glove.6B.100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "_ = glove2word2vec('data/glove.6B.100d.txt','data/glove2word2vec.6B.100d.txt')\n",
    "model = KeyedVectors.load_word2vec_format('data/glove2word2vec.6B.100d.txt')\n",
    "\n",
    "def word2vec_get(seg):\n",
    "    \"\"\"\n",
    "    get wordVec of seg with glove\n",
    "    \"\"\"\n",
    "    vector = np.zeros([1,100])\n",
    "    size = len(seg)\n",
    "    for word in seg:\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except KeyError:\n",
    "            size -= 1\n",
    "    vector /= size\n",
    "    vector_norm = np.linalg.norm(vector / size,axis=1,keepdims=True)\n",
    "    return vector / vector_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-329ac26b571f>:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vector += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros([len(qlist), 100])\n",
    "for cur in range(X.shape[0]):\n",
    "    X[cur] = word2vec_get(qlist[cur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5results_emb(input_q, K=5):\n",
    "    \"\"\"\n",
    "    similar to the above ones, except that we use word2vec\n",
    "    \"\"\"\n",
    "    if not input_q or type(input_q)!= type(\"\"):\n",
    "        print(\"input error\")\n",
    "        return\n",
    "    input_q = text_normalizer.normalize_text(input_q)\n",
    "    candidate_docs = get_candidate_docs(inverted_idx, input_q)\n",
    "    if not candidate_docs:\n",
    "        print(\"no answer error\")\n",
    "        return\n",
    "    q_vec = word2vec_get(input_q)\n",
    "    top_k_indices, top_idx = [], []\n",
    "    for i in candidate_docs:\n",
    "        doc_vec = X[i,:]\n",
    "        similarity = 1 - cosdist(doc_vec, q_vec)\n",
    "        similarity = 0 if np.isnan(similarity) else similarity\n",
    "        if len(top_k_indices) == K:\n",
    "            heapq.heappushpop(top_k_indices,(similarity, i))\n",
    "        else:\n",
    "            heapq.heappush(top_k_indices, (similarity, i))\n",
    "    top_idxs = sorted(top_k_indices, reverse=True)\n",
    "    _, top_idxs = zip(*top_idxs[::-1])\n",
    "    return [alist[id] for id in top_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the encyclical', 'George Washington', 'the colonies first rejected the authority of the Parliament to govern them without representation', '1985', 'Andrew Jackson']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-329ac26b571f>:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vector += model.wv[word]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(top5results_emb(\"who is the president of THE UNITED STATES NOW???\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
